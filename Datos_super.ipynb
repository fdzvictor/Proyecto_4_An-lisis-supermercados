{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING SUPERMERCADOS:\n",
    "\n",
    "## OBJETIVOS:\n",
    "- Scrapear la página web de FACUA para conseguir un EDA sobre la variabilidad de los precios de tres productos principales: Aceite de Oliva, Aceite de Girasol y Leche.\n",
    "- La comparación ha de ser entre los supermercados Alcampo, Carrefour, Dia, Eroski, Hipercor y Mercadona.\n",
    "- Análisis de la Evolución de Precios\n",
    "- Detección de Anomalías \n",
    "- Análisis de la Dispersión de Precios\n",
    "- Comparación de Precios Promedio   \n",
    "- Crear una base de datos SQL con la información.\n",
    "- Visualización de datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías que necesitamos\n",
    "# !pip install selenium \n",
    "# !pip install bs4\n",
    "# !pip install requests\n",
    "# !pip install webdriver-manager\n",
    "# !pip install pandas\n",
    "# !pip install numpyç\n",
    "# !pip install warnings\n",
    "# !pip install tqdm\n",
    "# Librerías de extracción de datos\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Importaciones:\n",
    "# Beautifulsoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Requests\n",
    "import requests\n",
    "\n",
    "# Importar librerías para automatización de navegadores web con Selenium\n",
    "# -----------------------------------------------------------------------\n",
    "from selenium import webdriver  # Selenium es una herramienta para automatizar la interacción con navegadores web.\n",
    "from webdriver_manager.chrome import ChromeDriverManager  # ChromeDriverManager gestiona la instalación del controlador de Chrome.\n",
    "from selenium.webdriver.common.keys import Keys  # Keys es útil para simular eventos de teclado en Selenium.\n",
    "from selenium.webdriver.support.ui import Select  # Select se utiliza para interactuar con elementos <select> en páginas web.\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException # Excepciones comunes de selenium que nos podemos encontrar \n",
    "\n",
    "\n",
    "# Importar librerías para pausar la ejecución\n",
    "# -----------------------------------------------------------------------\n",
    "from time import sleep  # Sleep se utiliza para pausar la ejecución del programa por un número de segundos.\n",
    "\n",
    "# Librerías para tratamiento de datos\n",
    "# -----------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Librerias de soporte\n",
    "#-------------------------------------------------------------------------\n",
    "import warnings\n",
    "import sys\n",
    "from src import soporte\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_supermercados = [\"alcampo\", \"carrefour\", \"dia\", \"eroski\", \"hipercor\", \"mercadona\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accedemos a la página web\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://super.facua.org/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accedemos al supermercado:\n",
    "driver = webdriver.Chrome()\n",
    "url = \"https://super.facua.org/\"\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "sleep(2)\n",
    "\n",
    "try:\n",
    "    driver.find_element(\"xpath\",'//*[@id=\"rcc-confirm-button\"]').click()\n",
    "    print(\"Botón de aceptar cookies clicado con éxito.\")\n",
    "except:\n",
    "    print(\"No se pudo encontrar el botón de aceptar cookies o hacer clic en él.\")\n",
    "\n",
    "# Automatically scroll the page\n",
    "\n",
    "# Scroll down\n",
    "driver.execute_script(f\"window.scrollTo(0, {0,100});\")\n",
    "sleep(2)\n",
    "\n",
    "#Click Mercadona\n",
    "driver.find_element(\"xpath\",'/html/body/section[1]/div/div[2]/div[1]/div/div[2]/div/a').click()\n",
    "sleep(5)\n",
    "\n",
    "\n",
    "#Scrapeamos aceite de girasol\n",
    "# Scroll\n",
    "driver.execute_script(f\"window.scrollTo(0, {0,100});\")\n",
    "sleep(2)\n",
    "#Click Aceite girasol\n",
    "driver.find_element(\"xpath\",'/html/body/section[1]/div/div[2]/div[1]/div/div[2]/div/a').click()\n",
    "sleep(1)\n",
    "#Aquí hay que scrapear todos los links de aceite de girasol !\n",
    "get_url1 = driver.current_url\n",
    "print(\"La url del aceite de girasol es:\"+str(get_url1))\n",
    "\n",
    "driver.back()\n",
    "sleep(3)\n",
    "\n",
    "#Click aceite de oliva\n",
    "driver.find_element(\"xpath\",\"/html/body/section[1]/div/div[2]/div[2]/div/div[2]/div/a\").click()\n",
    "sleep(1)\n",
    "#Scrapeamos aceites de oliva\n",
    "get_url2 = driver.current_url\n",
    "print(\"La url del aceite de oliva es:\"+str(get_url2))\n",
    "\n",
    "driver.back()\n",
    "sleep(3)\n",
    "\n",
    "#Click leche\n",
    "driver.find_element(\"xpath\",'/html/body/section[1]/div/div[2]/div[3]/div/div[2]/div/a').click()\n",
    "sleep(1)\n",
    "#Scrapeamos leche\n",
    "get_url3 = driver.current_url\n",
    "print(\"La url de la leche es:\"+str(get_url3))\n",
    "driver.back()\n",
    "sleep(3)\n",
    "\n",
    "#Cerran2\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un diccionario con los súpers y sus productos\n",
    "diccionario_supers = {}\n",
    "for supermercado in lista_supermercados:\n",
    "    diccionario_gral = soporte.scrapeo_por_supermercado(supermercado)\n",
    "    diccionario_supers[supermercado] = diccionario_gral\n",
    "\n",
    "enlaces = [url for productos in diccionario_supers.values() for url in productos.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sacamos los valores interiores\n",
    "def aplicar_subproductos_a_productos(lista_enlaces_de_los_productos):\n",
    "    dic_subs = {}\n",
    "    for url in lista_enlaces_de_los_productos:\n",
    "        url_producto = url\n",
    "        res_producto = requests.get(url_producto)\n",
    "\n",
    "        sopa_producto = BeautifulSoup(res_producto.content,\"html.parser\")\n",
    "\n",
    "        lista_subproductos = []\n",
    "        enlace_historico = sopa_producto.findAll(\"a\", class_=\"btn-unirme btn-verde inline-block inline-block bg-primary border-primary font-semibold rounded-full\")\n",
    "\n",
    "# Extrae el valor del atributo href\n",
    "        for valores in enlace_historico:\n",
    "            lista_subproductos.append(valores.get(\"href\"))\n",
    "        \n",
    "       \n",
    "        dic_subs[url] = lista_subproductos\n",
    "    return dic_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sublista = aplicar_subproductos_a_productos(enlaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sustituimos los productos en el diccionario por los subproductos\n",
    "\n",
    "for sup in diccionario_supers:\n",
    "    for cat in diccionario_supers[sup]: \n",
    "        print(sup,cat)\n",
    "        diccionario_supers[sup][cat] = sublista[diccionario_supers[sup][cat]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos nuestro diccionario\n",
    "diccionario_supers\n",
    "\n",
    "with open(\"sample.json\", \"w\") as outfile: \n",
    "    json.dump(diccionario_supers, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sacar_historico(url_sproducto):\n",
    "\n",
    "#Sacar histórico de precios, empecemos con una url: \n",
    "    \n",
    "    res_producto = requests.get(url_sproducto)\n",
    "    print(res_producto)\n",
    "\n",
    "#Creamos bs4, buscamos aquellos elementos que sean tablas y seleccionamos la que queremos en concreto\n",
    "    sopa_producto = BeautifulSoup(res_producto.content,\"html.parser\")\n",
    "    tablas = sopa_producto.find_all(\"table\")\n",
    "\n",
    "    tablasweb = []\n",
    "    for n in tablas:\n",
    "        tablasweb.append(n)\n",
    "##Puesto que hay webs que tienen dos tablas, cogemos siempre la de históricos, la última.\n",
    "    try:\n",
    "        tabla = tablasweb[-1]\n",
    "  \n",
    "        lista_filas_producto = tabla.findAll(\"tr\")\n",
    "        print(f\" la tabla tiene {len(lista_filas_producto)} filas\")\n",
    "\n",
    "# Creamos un diccionario llamado diccionario_producto para almacenar todos los resultados\n",
    "# Iniciamos un bucle 'for' para iterar a través de la lista_filas_producto a partir de la segunda fila\n",
    "        diccionario_producto = {}\n",
    "        diccionario_variaciones = {}\n",
    "        for fila in lista_filas_producto[1:]:\n",
    "    # Para cada 'fila', extraemos el texto y los datos\n",
    "            fila_texto = fila.findAll(\"td\")\n",
    "    #Creamos un diccionario con cada valor a través de la lista anteriormente generada\n",
    "            diccionario_producto[fila_texto[0].getText()] = fila_texto[1].getText()\n",
    "            diccionario_variaciones [fila_texto[0].getText()] = fila_texto[2].getText()\n",
    "\n",
    "    # Imprimimos los resultados obtenidos después de iterar por la lista.\n",
    "        print(f\"Los resultados de iterar por la lista son:\\n {diccionario_producto}\")\n",
    "\n",
    "#Creamos el df del histórico del producto\n",
    "        df_producto = pd.DataFrame(diccionario_producto.values(), index = diccionario_variaciones.keys())\n",
    "\n",
    "\n",
    "#Creamos columnas para la variación de precio y la variación porcentual    \n",
    "        df_producto[\"variaciones\"] = diccionario_variaciones.values()\n",
    "        \n",
    "#Creamos columnas para el supermercado y el producto       \n",
    "        split_url = (url_sproducto.rsplit(\"/\"))\n",
    "        df_producto[\"Supermercado\"] = split_url[3]\n",
    "        df_producto[\"Categoría\"] = split_url[4].replace(\"-\",\" \")\n",
    "        df_producto[\"Nombre_producto\"] = split_url[-2].replace(\"-\",\" \")\n",
    "\n",
    "#Si la columna \"Variación\" no tiene valores, generamos dos columnas vacías, para que cuadre todo el df final\n",
    "        try:\n",
    "            df_producto[[\"Variación\",\"Variación_porcentual\"]] = df_producto[\"variaciones\"].str.rsplit(\"(\", expand = True)\n",
    "            df_producto[\"Variación_porcentual\"] = df_producto[\"Variación_porcentual\"].str.split(\")\")\n",
    "            df_producto.drop(columns = \"variaciones\",inplace = True)\n",
    "            ## Para estandarizar columnas dejamos los \"=\" como \"None\"\n",
    "            df_producto['Variación'] = df_producto['Variación'].apply(lambda x: None if x == \"=\" else x)\n",
    "\n",
    "            \n",
    "\n",
    "        except ValueError:\n",
    "            df_producto[\"Variación\"] = \"None\"\n",
    "            df_producto[\"Variación_porcentual\"] = \"None\"\n",
    "            df_producto.drop(columns = \"variaciones\",inplace = True)\n",
    "\n",
    "        return df_producto \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_supers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Una vez construido el dataframe, iteramos sobre todos los elementos       \n",
    "          \n",
    "\n",
    "df_grande = pd.DataFrame()\n",
    "\n",
    "for categoria in tqdm(diccionario_supers.values()):\n",
    "     for lista_link in categoria.values():\n",
    "        for url in lista_link:\n",
    "            df_item = soporte.sacar_historico(url)\n",
    "            df_grande = pd.concat([df_grande,df_item])\n",
    "\n",
    "df_grande.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos todas las observaciones en un csv\n",
    "df_grande.to_csv('Productos_supermercados.csv', encoding = \"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
